  0%|          | 0/15 [00:00<?, ?it/s]/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
  7%|▋         | 1/15 [01:15<17:33, 75.28s/it] 13%|█▎        | 2/15 [01:19<07:18, 33.75s/it] 20%|██        | 3/15 [01:20<03:42, 18.57s/it] 27%|██▋       | 4/15 [01:20<02:05, 11.43s/it] 33%|███▎      | 5/15 [01:21<01:14,  7.48s/it] 40%|████      | 6/15 [01:21<00:46,  5.12s/it] 47%|████▋     | 7/15 [01:22<00:28,  3.61s/it] 53%|█████▎    | 8/15 [01:22<00:18,  2.61s/it] 60%|██████    | 9/15 [01:23<00:11,  1.94s/it] 67%|██████▋   | 10/15 [01:23<00:07,  1.49s/it] 73%|███████▎  | 11/15 [01:24<00:04,  1.19s/it] 80%|████████  | 12/15 [01:24<00:02,  1.02it/s] 87%|████████▋ | 13/15 [01:25<00:01,  1.21it/s] 93%|█████████▎| 14/15 [01:25<00:00,  1.36it/s]100%|██████████| 15/15 [01:26<00:00,  1.51it/s]100%|██████████| 15/15 [01:26<00:00,  5.76s/it]
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
  0%|          | 0/15 [00:00<?, ?it/s]/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
  7%|▋         | 1/15 [01:02<14:35, 62.55s/it] 13%|█▎        | 2/15 [01:06<06:04, 28.05s/it] 20%|██        | 3/15 [01:07<03:06, 15.55s/it] 27%|██▋       | 4/15 [01:07<01:46,  9.68s/it] 33%|███▎      | 5/15 [01:08<01:04,  6.43s/it] 40%|████      | 6/15 [01:09<00:40,  4.49s/it] 47%|████▋     | 7/15 [01:09<00:25,  3.25s/it] 53%|█████▎    | 8/15 [01:10<00:17,  2.43s/it] 60%|██████    | 9/15 [01:11<00:11,  1.89s/it] 67%|██████▋   | 10/15 [01:11<00:07,  1.51s/it] 73%|███████▎  | 11/15 [01:12<00:05,  1.26s/it] 80%|████████  | 12/15 [01:13<00:03,  1.10s/it] 87%|████████▋ | 13/15 [01:14<00:01,  1.03it/s] 93%|█████████▎| 14/15 [01:14<00:00,  1.13it/s]100%|██████████| 15/15 [01:15<00:00,  1.21it/s]100%|██████████| 15/15 [01:15<00:00,  5.03s/it]
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
  0%|          | 0/15 [00:00<?, ?it/s]/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
  7%|▋         | 1/15 [00:41<09:42, 41.64s/it] 13%|█▎        | 2/15 [00:45<04:13, 19.50s/it] 20%|██        | 3/15 [00:46<02:13, 11.12s/it] 27%|██▋       | 4/15 [00:47<01:19,  7.19s/it] 33%|███▎      | 5/15 [00:49<00:50,  5.02s/it] 40%|████      | 6/15 [00:50<00:33,  3.72s/it] 47%|████▋     | 7/15 [00:51<00:23,  2.89s/it] 53%|█████▎    | 8/15 [00:52<00:16,  2.34s/it] 60%|██████    | 9/15 [00:53<00:11,  1.98s/it] 67%|██████▋   | 10/15 [00:55<00:08,  1.73s/it] 73%|███████▎  | 11/15 [00:56<00:06,  1.56s/it] 80%|████████  | 12/15 [00:57<00:04,  1.44s/it] 87%|████████▋ | 13/15 [00:58<00:02,  1.36s/it] 93%|█████████▎| 14/15 [00:59<00:01,  1.31s/it]100%|██████████| 15/15 [01:00<00:00,  1.28s/it]100%|██████████| 15/15 [01:00<00:00,  4.06s/it]
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
  0%|          | 0/15 [00:00<?, ?it/s]/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
  7%|▋         | 1/15 [00:40<09:33, 40.93s/it]  7%|▋         | 1/15 [00:44<10:16, 44.03s/it]
Traceback (most recent call last):
  File "/home/c-tadimeti/language-model-basics/cs336_systems/benchmarking_script.py", line 227, in <module>
    profile(cfg)
    ~~~~~~~^^^^^
  File "/home/c-tadimeti/language-model-basics/cs336_systems/benchmarking_script.py", line 104, in profile
    logits = model(xb)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 574, in _fn
    return fn(*args, **kwargs)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/c-tadimeti/language-model-basics/cs336-basics/cs336_basics/transformer_modules.py", line 411, in forward
    x = block(x)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/c-tadimeti/language-model-basics/cs336-basics/cs336_basics/transformer_modules.py", line 339, in forward
    def forward(self, x: torch.Tensor) -> torch.Tensor:
  File "/home/c-tadimeti/language-model-basics/cs336-basics/cs336_basics/transformer_modules.py", line 343, in torch_dynamo_resume_in_forward_at_343
    x = self.mha(x) + res
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
    return fn(*args, **kwargs)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py", line 1184, in forward
    return compiled_fn(full_args)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 310, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
        compiled_fn, args_, disable_amp=disable_amp, steal_args=True
    )
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ~^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1585, in forward
    fw_outs = call_func_at_runtime_with_args(
        CompiledFunction.compiled_fw,
        args,
        disable_amp=disable_amp,
    )
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ~^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 490, in wrapper
    return compiled_fn(runtime_args)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 672, in inner_fn
    outs = compiled_fn(args)
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py", line 466, in __call__
    return self.current_callable(inputs)
           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/home/c-tadimeti/language-model-basics/.venv/lib/python3.13/site-packages/torch/_inductor/utils.py", line 2128, in run
    return model(new_inputs)
  File "/tmp/torchinductor_c-tadimeti/sz/cszdgpumjost2uk3wk4fvatpyyby4ezgzg4kjrqzyhesla6hdb6j.py", line 407, in call
    buf10 = empty_strided_cuda((1, 4096, 10240), (41943040, 1, 4096), torch.float32)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 79.06 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
