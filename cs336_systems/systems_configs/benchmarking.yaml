# Unspecifying checkpoint_directory to avoid checkpointing

#experiment_name: "owt"

# Model architecture parameters
vocab_size: 10000
context_length: 128

d_model: 768
num_heads: 32
num_layers: 12
d_ff: 3072

batch_size: 4
max_iters: 25000

# Learning rate schedule parameters
max_lr: 0.005
min_lr: 0.0005
warmup_iters_frac: 0.2
cosine_cycle_frac: 0.7

# Gradient parameters 
gradient_accumulation_steps: 1
max_grad_norm: 1.0 # For clippping

# Rope 
theta: 10000

# Optimizer hyperparameters
optimizer: "adamw"    # options: "sgd" or "adamw"
beta1: 0.9
beta2: 0.999
eps: 1e-8
weight_decay: 0.001


# Logging & saving intervals
log_interval: 500
val_interval: 500
save_interval: 5000000000

# Profiling parameters
profile_warmup: 5
profile_measurement_steps: 10
profile_component: "forward/backward" # either "forward" or "forward/backward"