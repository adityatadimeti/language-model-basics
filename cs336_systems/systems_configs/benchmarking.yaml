
train_data: "random"
val_data: "random"
checkpoint_directory: "/data/c-tadimeti/data/model_checkpoints"
# resume: "checkpoints/transformer_lm.ckpt"  # uncomment to resume from a specific checkpoint

#experiment_name: "owt"

# Model architecture parameters
vocab_size: 32000
context_length: 256

d_model: 1024
num_heads: 32
num_layers: 8
d_ff: 1344

batch_size: 64
max_iters: 25000

# Learning rate schedule parameters
max_lr: 0.005
min_lr: 0.0005
warmup_iters_frac: 0.2
cosine_cycle_frac: 0.7

# Gradient parameters 
gradient_accumulation_steps: 1
max_grad_norm: 1.0 # For clippping

# Rope 
theta: 10000

# Optimizer hyperparameters
optimizer: "adamw"    # options: "sgd" or "adamw"
beta1: 0.9
beta2: 0.999
eps: 1e-8
weight_decay: 0.001


# Logging & saving intervals
log_interval: 500
val_interval: 500
save_interval: 5000000000


