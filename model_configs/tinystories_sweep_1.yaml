batch_size: 512
beta1: 0.9
beta2: 0.999
checkpoint_path: model_checkpoints/transformer_lm.ckpt
context_length: 256
cosine_cycle_frac: 0.8
d_ff: 1344
d_model: 512
eps: 1e-8
experiment_name: test
log_interval: 10
max_grad_norm: 1.0
max_iters: 2500
max_lr: 0.0005
min_lr: 5.0e-06
num_heads: 16
num_layers: 6
optimizer: adamw
save_interval: 500
theta: 10000
train_data: data/tinystories_tokenized_ids_train.npy
val_data: data/tinystories_tokenized_ids_validation.npy
val_interval: 50
vocab_size: 10000
warmup_iters_frac: 0.05
weight_decay: 0.01
