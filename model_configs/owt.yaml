train_data: "data/owt_tokenized_ids.npy"
val_data: "data/owt_valid_ids.npy"
checkpoint_path: "model_checkpoints/owt_transformer_lm.ckpt"
# resume: "checkpoints/transformer_lm.ckpt"  # uncomment to resume from a specific checkpoint

experiment_name: "owt"


vocab_size: 32000
context_length: 256

d_model: 1024
num_heads: 32
num_layers: 8
d_ff: 1344

batch_size: 128
max_iters: 2500

# Learning rate schedule
max_lr: 0.001
min_lr: 0.00001
warmup_iters_frac: 0.05
cosine_cycle_frac: 0.8

# Rope 
theta: 10000

# Optimizer hyperparameters
optimizer: "adamw"    # options: "sgd" or "adamw"
beta1: 0.9
beta2: 0.999
eps: 1e-8
weight_decay: 0.01

# Gradient clipping
max_grad_norm: 1.0

# Logging & saving intervals
log_interval: 10
val_interval: 50
save_interval: 500


