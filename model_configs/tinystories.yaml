train_data: "data/tinystories_tokenized_ids_train.npy"
val_data: "data/tinystories_tokenized_ids_validation.npy"
checkpoint_path: "model_checkpoints/transformer_lm.ckpt"
# resume: "checkpoints/transformer_lm.ckpt"  # uncomment to resume from a specific checkpoint

experiment_name: "test"


vocab_size: 10000
context_length: 256

d_model: 512
num_heads: 4
num_layers: 1
d_ff: 1344

batch_size: 32
max_iters: 5

# Learning rate schedule
max_lr: 0.001
min_lr: 0.00001
warmup_iters: 500
cosine_cycle_iters: 8000

# Rope
theta: 10000

# Optimizer hyperparameters
optimizer: "adamw"    # options: "sgd" or "adamw"
beta1: 0.9
beta2: 0.999
eps: 1e-8
weight_decay: 0.01

# Gradient clipping
max_grad_norm: 1.0

# Logging & saving intervals
log_interval: 1
val_interval: 1
save_interval: 1

device: "cpu"  # "cpu" or "cuda"

